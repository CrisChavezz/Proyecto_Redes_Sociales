# -*- coding: utf-8 -*-
"""Proyecto_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eAnQG8zHxkzLCuLEegffXO0KxIybUr6K

# Proyecto Análisis de Tendencias en Redes Sociales

## Equipo 2:

### Camila

### Alonso

### Cristofer

### Recolección de datos
"""

# Conexión con google drive para la recolección de datos
from google.colab import drive
drive.mount('/content/drive')

#Intalación de librerias
!pip install pandas
!pip install numpy
!pip install matplotlib
!pip install spacy
!pip install seaborn
!python -m spacy download es_core_news_sm
!pip install nltk
!pip install json
!pip install wordcloud
!pip install deep_translator
!pip install googletrans==4.0.0-rc1
!pip install googletrans==3.1.0a0

#Importacion de librerias
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import spacy
import nltk
import json
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

#Cargar datos necesarios para nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')

# Leer archivos .json
DFFB = pd.read_json('/content/drive/MyDrive/Proyecto/facebook_data.json', lines=False)
DFIN = pd.read_json('/content/drive/MyDrive/Proyecto/instagram_data.json', lines=False)
DFX = pd.read_json('/content/drive/MyDrive/Proyecto/twitter_data.json', lines=False)

"""### Preprocesamiento de datos"""

# Preprocesamiento de datos: Tokenización, Lematización, Eliminación de StopWords y Normalización de textos
# ---------------------Usando la base de datos de Facebook--------------------
# Importar Bibliotecas Necesarias
import pandas as pd
import re
import spacy
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

#-----------------Recursos necesarios para aplicar el preprocesamiento en español del texto-----------------
# Descargar recursos necesarios de NLTK
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

# Cargar el modelo de spaCy en español
nlp = spacy.load("es_core_news_sm")

#------------Aplicar una limpieza de los datos-----------------
#Crear una funcion para limpiar datos

def limpiar_datos(texto):
  """
  Normalizacion del texto: conversión minusculas y eliminar carcateres especiales.
  """
  # Eliminar caracteres especiales y conversión a minúsculas
  texto = texto.lower()                                    #Convierto a minúsculas
  texto = re.sub(r'[^a-zA-ZáéíóúñüÁÉÍÓÚÑÜ\s]', '', texto)  #Elimino caracteres especiales
  texto = re.sub(r'[^\w\s+]', ' ', texto)                  #Elimino signos de puntuación
  texto = re.sub(r'\d+', ' ', texto)                       #Elimino números
  # texto = re.sub(r'\s+', ' ', texto)                     #Elimino espacios en blanco
  texto = texto.strip()                                    #Elimino espacios en blanco al inicio y al final del texto extras
  return texto

#---------Aplicación del Preprocesamiento de datos-----------------

def preprocesamiento_datos(texto):
  """
  Función para aplicar el preprocesamiento de datos, tokenización, lematización, eliminación de stopwords y normalización de textos.
  """
  # Aplicar la función de limpieza de datos
  texto = limpiar_datos(texto)

  # Tokenización
  tokens = word_tokenize(texto, language='spanish')

  # Eliminar stopwords en español
  stop_words = set(stopwords.words('spanish'))
  tokens_sin_stop = [word for word in tokens if word not in stop_words]

  # Lematización con spaCy
  doc = nlp(' '.join(tokens_sin_stop))
  tokens_lematizados = [token.lemma_ for token in doc]

  # Unir los tokens procesados
  texto_procesado = ' '.join(tokens_lematizados)

  return texto_procesado

#----------Pruebas con textos en español--------
texto = DFFB['contenido'].to_string()

# Aplicar procesamiento
DFFB['Texto_Procesado'] = DFFB['contenido'].apply(preprocesamiento_datos)
# print("Texto original: \n", texto)
# print("Texto procesado: ", texto_procesado)

# print(FB['Texto_Procesado'],FB['contenido'])
print(DFFB)

DFFB

# Preprocesamiento de datos: Tokenización, Lematización, Eliminación de StopWords y Normalización de textos
# ---------------------Usando la base de datos de Intagram--------------------
# Importar Bibliotecas Necesarias
import pandas as pd
import re
import spacy
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

#-----------------Recursos necesarios para aplicar el preprocesamiento en español del texto-----------------
# Descargar recursos necesarios de NLTK
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

# Cargar el modelo de spaCy en español
nlp = spacy.load("es_core_news_sm")

#------------Aplicar una limpieza de los datos-----------------
#Crear una funcion para limpiar datos

def limpiar_datos(texto):
  """
  Normalizacion del texto: conversión minusculas y eliminar carcateres especiales.
  """
  # Eliminar caracteres especiales y conversión a minúsculas
  texto = texto.lower()                                    #Convierto a minúsculas
  texto = re.sub(r'[^a-zA-ZáéíóúñüÁÉÍÓÚÑÜ\s]', '', texto)  #Elimino caracteres especiales
  texto = re.sub(r'[^\w\s+]', ' ', texto)                  #Elimino signos de puntuación
  texto = re.sub(r'\d+', ' ', texto)                       #Elimino números
  # texto = re.sub(r'\s+', ' ', texto)                     #Elimino espacios en blanco
  texto = texto.strip()                                    #Elimino espacios en blanco al inicio y al final del texto extras
  return texto

#---------Aplicación del Preprocesamiento de datos-----------------

def preprocesamiento_datos(texto):
  """
  Función para aplicar el preprocesamiento de datos, tokenización, lematización, eliminación de stopwords y normalización de textos.
  """
  # Aplicar la función de limpieza de datos
  texto = limpiar_datos(texto)

  # Tokenización
  tokens = word_tokenize(texto, language='spanish')

  # Eliminar stopwords en español
  stop_words = set(stopwords.words('spanish'))
  tokens_sin_stop = [word for word in tokens if word not in stop_words]

  # Lematización con spaCy
  doc = nlp(' '.join(tokens_sin_stop))
  tokens_lematizados = [token.lemma_ for token in doc]

  # Unir los tokens procesados
  texto_procesado = ' '.join(tokens_lematizados)

  return texto_procesado

#----------Pruebas con textos en español--------
texto = DFIN['contenido'].to_string()

# Aplicar procesamiento
DFIN['Texto_Procesado'] = DFIN['contenido'].apply(preprocesamiento_datos)
# print("Texto original: \n", texto)
# print("Texto procesado: ", texto_procesado)

# print(FB['Texto_Procesado'],FB['contenido'])
print(DFIN)

DFIN

# Preprocesamiento de datos: Tokenización, Lematización, Eliminación de StopWords y Normalización de textos
# ---------------------Usando la base de datos de Facebook--------------------
# Importar Bibliotecas Necesarias
import pandas as pd
import re
import spacy
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

#-----------------Recursos necesarios para aplicar el preprocesamiento en español del texto-----------------
# Descargar recursos necesarios de NLTK
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

# Cargar el modelo de spaCy en español
nlp = spacy.load("es_core_news_sm")

#------------Aplicar una limpieza de los datos-----------------
#Crear una funcion para limpiar datos

def limpiar_datos(texto):
  """
  Normalizacion del texto: conversión minusculas y eliminar carcateres especiales.
  """
  # Eliminar caracteres especiales y conversión a minúsculas
  texto = texto.lower()                                    #Convierto a minúsculas
  texto = re.sub(r'[^a-zA-ZáéíóúñüÁÉÍÓÚÑÜ\s]', '', texto)  #Elimino caracteres especiales
  texto = re.sub(r'[^\w\s+]', ' ', texto)                  #Elimino signos de puntuación
  texto = re.sub(r'\d+', ' ', texto)                       #Elimino números
  # texto = re.sub(r'\s+', ' ', texto)                     #Elimino espacios en blanco
  texto = texto.strip()                                    #Elimino espacios en blanco al inicio y al final del texto extras
  return texto

#---------Aplicación del Preprocesamiento de datos-----------------

def preprocesamiento_datos(texto):
  """
  Función para aplicar el preprocesamiento de datos, tokenización, lematización, eliminación de stopwords y normalización de textos.
  """
  # Aplicar la función de limpieza de datos
  texto = limpiar_datos(texto)

  # Tokenización
  tokens = word_tokenize(texto, language='spanish')

  # Eliminar stopwords en español
  stop_words = set(stopwords.words('spanish'))
  tokens_sin_stop = [word for word in tokens if word not in stop_words]

  # Lematización con spaCy
  doc = nlp(' '.join(tokens_sin_stop))
  tokens_lematizados = [token.lemma_ for token in doc]

  # Unir los tokens procesados
  texto_procesado = ' '.join(tokens_lematizados)

  return texto_procesado

#----------Pruebas con textos en español--------
texto = DFX['contenido'].to_string()

# Aplicar procesamiento
DFX['Texto_Procesado'] = DFX['contenido'].apply(preprocesamiento_datos)
# print("Texto original: \n", texto)
# print("Texto procesado: ", texto_procesado)

# print(FB['Texto_Procesado'],FB['contenido'])
print(DFX)

DFX

"""### Análisis Transitorio"""

import nltk
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import pandas as pd

# Descargar recursos necesarios
nltk.download('punkt')
nltk.download('stopwords')

# Convertir columna a string
texto = " ".join(DFFB['Texto_Procesado'].astype(str))

# Tokenización
tokens = nltk.word_tokenize(texto.lower())

# Eliminación de stop words
stop_words = set(nltk.corpus.stopwords.words('spanish'))
tokens_filtrados_Fb = [word for word in tokens if word.isalnum() and word not in stop_words]

# Contar la frecuencia de las palabras
frecuencia_palabras = Counter(tokens_filtrados_Fb)

# Tomar solo las 20 palabras más frecuentes
palabras_comunes = frecuencia_palabras.most_common(30)
palabras, frecuencias = zip(*palabras_comunes)

# Visualización de las palabras más frecuentes
plt.figure(figsize=(20, 10))
plt.bar(palabras, frecuencias, color='skyblue')
plt.xticks(rotation=45)  # Rotar etiquetas para mejor lectura
plt.title('30 Palabras más frecuentes en Facebook')
plt.xlabel('Palabras')
plt.ylabel('Frecuencia')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Generar una nube de palabras
wordcloud = WordCloud(width=2000, height=1000, background_color='white', colormap='viridis').generate_from_frequencies(frecuencia_palabras)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Ocultar ejes
plt.show()

import nltk
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import pandas as pd

# Descargar recursos necesarios
nltk.download('punkt')
nltk.download('stopwords')

# Convertir columna a string
texto = " ".join(DFIN['Texto_Procesado'].astype(str))

# Tokenización
tokens = nltk.word_tokenize(texto.lower())

# Eliminación de stop words
stop_words = set(nltk.corpus.stopwords.words('spanish'))
tokens_filtrados_In = [word for word in tokens if word.isalnum() and word not in stop_words]

# Contar la frecuencia de las palabras
frecuencia_palabras = Counter(tokens_filtrados_In)

# Tomar solo las 20 palabras más frecuentes
palabras_comunes = frecuencia_palabras.most_common(30)
palabras, frecuencias = zip(*palabras_comunes)

# Visualización de las palabras más frecuentes
plt.figure(figsize=(20, 10))
plt.bar(palabras, frecuencias, color='blue')
plt.xticks(rotation=45)  # Rotar etiquetas para mejor lectura
plt.title('30 Palabras más frecuentes en Intagram')
plt.xlabel('Palabras')
plt.ylabel('Frecuencia')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Generar una nube de palabras
wordcloud = WordCloud(width=2000, height=1000, background_color='white', colormap='viridis').generate_from_frequencies(frecuencia_palabras)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Ocultar ejes
plt.show()

import nltk
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import pandas as pd

# Descargar recursos necesarios
nltk.download('punkt')
nltk.download('stopwords')

# Convertir columna a string
texto = " ".join(DFX['Texto_Procesado'].astype(str))

# Tokenización
tokens = nltk.word_tokenize(texto.lower())

# Eliminación de stop words
stop_words = set(nltk.corpus.stopwords.words('spanish'))
tokens_filtrados_X = [word for word in tokens if word.isalnum() and word not in stop_words]

# Contar la frecuencia de las palabras
frecuencia_palabras = Counter(tokens_filtrados_X)

# Tomar solo las 20 palabras más frecuentes
palabras_comunes = frecuencia_palabras.most_common(30)
palabras, frecuencias = zip(*palabras_comunes)

# Visualización de las palabras más frecuentes
plt.figure(figsize=(20, 10))
plt.bar(palabras, frecuencias, color='red')
plt.xticks(rotation=45)  # Rotar etiquetas para mejor lectura
plt.title('30 Palabras más frecuentes en X')
plt.xlabel('Palabras')
plt.ylabel('Frecuencia')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Generar una nube de palabras
wordcloud = WordCloud(width=2000, height=1000, background_color='white', colormap='viridis').generate_from_frequencies(frecuencia_palabras)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Ocultar ejes
plt.show()

"""### N-gramas"""

from nltk.util import ngrams

print(tokens_filtrados_Fb)

# Crear bigramas (n-gramas de 2 palabras)
bigrams = list(ngrams(tokens_filtrados_Fb, 3))

# Contar la frecuencia de los bigramas
frecuencia_bigrams = Counter(bigrams)

# Visualización de los bigramas más frecuentes
bigrams_mas_frecuentes = frecuencia_bigrams.most_common(20)
bigrams_mas_frecuentes = { ' '.join(k): v for k, v in bigrams_mas_frecuentes }

plt.figure(figsize=(20, 10))
plt.bar(bigrams_mas_frecuentes.keys(), bigrams_mas_frecuentes.values(), color = 'skyblue')
plt.title('Bigrams más frecuentes de Facebook')
plt.xlabel('Bigrams')
plt.ylabel('Frecuencia')
plt.xticks(rotation=45)
plt.show()

from nltk.util import ngrams

print(tokens_filtrados_In)

# Crear bigramas (n-gramas de 2 palabras)
bigrams = list(ngrams(tokens_filtrados_In, 3))

# Contar la frecuencia de los bigramas
frecuencia_bigrams = Counter(bigrams)

# Visualización de los bigramas más frecuentes
bigrams_mas_frecuentes = frecuencia_bigrams.most_common(20)
bigrams_mas_frecuentes = { ' '.join(k): v for k, v in bigrams_mas_frecuentes }

plt.figure(figsize=(20, 10))
plt.bar(bigrams_mas_frecuentes.keys(), bigrams_mas_frecuentes.values(), color = 'blue')
plt.title('Bigrams más frecuentes de Intagram')
plt.xlabel('Bigrams')
plt.ylabel('Frecuencia')
plt.xticks(rotation=45)
plt.show()

from nltk.util import ngrams

print(tokens_filtrados_X)

# Crear bigramas (n-gramas de 2 palabras)
bigrams = list(ngrams(tokens_filtrados_X, 3))

# Contar la frecuencia de los bigramas
frecuencia_bigrams = Counter(bigrams)

# Visualización de los bigramas más frecuentes
bigrams_mas_frecuentes = frecuencia_bigrams.most_common(20)
bigrams_mas_frecuentes = { ' '.join(k): v for k, v in bigrams_mas_frecuentes }

plt.figure(figsize=(20, 10))
plt.bar(bigrams_mas_frecuentes.keys(), bigrams_mas_frecuentes.values(), color = 'red')
plt.title('Bigrams más frecuentes de X')
plt.xlabel('Bigrams')
plt.ylabel('Frecuencia')
plt.xticks(rotation=45)
plt.show()

"""### Longitud de documentos y distrinucion de palabras"""

#pip install seaborn
import nltk
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Descargar recursos necesarios
nltk.download('punkt')

# Convertir columna a string
textos = " ".join(DFFB['Texto_Procesado'].astype(str))

# Función para calcular la longitud de los documentos
def calcular_longitud(texto):
    tokens = nltk.word_tokenize(texto)
    return len(tokens)

# Crear un DataFrame con las longitudes de los documentos
# df = pd.DataFrame({'texto': [textos]})
DFFB['longitud'] = DFFB['Texto_Procesado'].apply(calcular_longitud)

# Mostrar el DataFrame
print(DFFB)

# Visualización de la distribución de la longitud de los documentos
plt.figure(figsize=(10, 5))
sns.histplot(DFFB['longitud'], bins=10, kde=False, color='skyblue')
plt.title('Distribución de la longitud de los documentos en Facebook')
plt.xlabel('Número de palabras')
plt.ylabel('Frecuencia')
plt.show()

#pip install seaborn
import nltk
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Descargar recursos necesarios
nltk.download('punkt')

# Convertir columna a string
textos = " ".join(DFIN['Texto_Procesado'].astype(str))

# Función para calcular la longitud de los documentos
def calcular_longitud(texto):
    tokens = nltk.word_tokenize(texto)
    return len(tokens)

# Crear un DataFrame con las longitudes de los documentos
# df = pd.DataFrame({'texto': [textos]})
DFIN['longitud'] = DFIN['Texto_Procesado'].apply(calcular_longitud)

# Mostrar el DataFrame
print(DFIN)

# Visualización de la distribución de la longitud de los documentos
plt.figure(figsize=(10, 5))
sns.histplot(DFIN['longitud'], bins=10, kde=False, color='pink')
plt.title('Distribución de la longitud de los documentos en Instagram')
plt.xlabel('Número de palabras')
plt.ylabel('Frecuencia')
plt.show()

#pip install seaborn
import nltk
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Descargar recursos necesarios
nltk.download('punkt')

# Convertir columna a string
textos = " ".join(DFX['Texto_Procesado'].astype(str))

# Función para calcular la longitud de los documentos
def calcular_longitud(texto):
    tokens = nltk.word_tokenize(texto)
    return len(tokens)

# Crear un DataFrame con las longitudes de los documentos
# df = pd.DataFrame({'texto': [textos]})
DFX['longitud'] = DFX['Texto_Procesado'].apply(calcular_longitud)

# Mostrar el DataFrame
print(DFX)

# Visualización de la distribución de la longitud de los documentos
plt.figure(figsize=(10, 5))
sns.histplot(DFX['longitud'], bins=10, kde=False, color = 'red')
plt.title('Distribución de la longitud de los documentos en X')
plt.xlabel('Número de palabras')
plt.ylabel('Frecuencia')
plt.show()

from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# Descargar recursos necesarios
nltk.download('punkt')
nltk.download('stopwords')

# Texto de ejemplo
texto =  " ".join(DFFB['contenido'].astype(str))

# Tokenización
tokens = nltk.word_tokenize(texto.lower())

# Eliminación de stop words
stop_words = set(nltk.corpus.stopwords.words('spanish'))
tokens_filtrados = [word for word in tokens if word.isalnum() and word not in stop_words]

# Contar la frecuencia de las palabras
frecuencia_palabras = Counter(tokens_filtrados)

frecuencia_palabras

# Visualización de la distribución de palabras
plt.figure(figsize=(10, 5))
sns.histplot(list(frecuencia_palabras.values()), bins=10, kde=False, color='skyblue')
plt.title('Distribución de la frecuencia de palabras en Facebook')
plt.xlabel('Frecuencia')
plt.ylabel('Número de palabras')
plt.show()

# Generar una nube de palabras
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(frecuencia_palabras)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# Descargar recursos necesarios
nltk.download('punkt')
nltk.download('stopwords')

# Texto de ejemplo
texto = " ".join(DFIN['contenido'].astype(str))

# Tokenización
tokens = nltk.word_tokenize(texto.lower())

# Eliminación de stop words
stop_words = set(nltk.corpus.stopwords.words('spanish'))
tokens_filtrados = [word for word in tokens if word.isalnum() and word not in stop_words]

# Contar la frecuencia de las palabras
frecuencia_palabras = Counter(tokens_filtrados)

frecuencia_palabras

# Visualización de la distribución de palabras
plt.figure(figsize=(10, 5))
sns.histplot(list(frecuencia_palabras.values()), bins=10, kde=False, color='pink')
plt.title('Distribución de la frecuencia de palabras en Intagram')
plt.xlabel('Frecuencia')
plt.ylabel('Número de palabras')
plt.show()

# Generar una nube de palabras
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(frecuencia_palabras)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# Descargar recursos necesarios
nltk.download('punkt')
nltk.download('stopwords')

# Texto de ejemplo
texto = " ".join(DFX['contenido'].astype(str))

# Tokenización
tokens = nltk.word_tokenize(texto.lower())

# Eliminación de stop words
stop_words = set(nltk.corpus.stopwords.words('spanish'))
tokens_filtrados = [word for word in tokens if word.isalnum() and word not in stop_words]

# Contar la frecuencia de las palabras
frecuencia_palabras = Counter(tokens_filtrados)

frecuencia_palabras

# Visualización de la distribución de palabras
plt.figure(figsize=(10, 5))
sns.histplot(list(frecuencia_palabras.values()), bins=10, kde=False, color="red")
plt.title('Distribución de la frecuencia de palabras en X')
plt.xlabel('Frecuencia')
plt.ylabel('Número de palabras')
plt.show()

# Generar una nube de palabras
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(frecuencia_palabras)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""### Análisis de sentimiento"""

!pip install deep_translator

#---------------- Obtener sentimiento dependiendo del comentario en cada DF------------------
from textblob import TextBlob
from deep_translator import GoogleTranslator

# Función para obtener el sentimiento
def obtener_sentimiento(texto):
    try:
        texto_traducido = GoogleTranslator(source='auto', target='en').translate(texto)
        polaridad = TextBlob(texto_traducido).sentiment.polarity
    except:
        # polaridad = TextBlob(texto).sentiment.polarity
        polaridad = 0
        # return 'Neutral'

    if polaridad > 0:
        return 'Positivo'
    elif polaridad == 0:
        return 'Positivo'
    else:
        return 'Negativo'

# Aplicar la función a la columna 'Texto_Procesado' y crear una nueva columna 'Sentimiento'
# DFFB.drop('Sentimiento', axis=1, inplace=True)
DFFB['Sentimiento'] = DFFB['contenido'].apply(obtener_sentimiento)
DFFB2 = DFFB[['Texto_Procesado','Sentimiento']]
DFFB

DFFB2

# DFFB2.to_csv('/content/drive/MyDrive/Proyecto/FB2.csv', index=False)

import spacy
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

# Cargar el modelo en español
nlp = spacy.load('es_core_news_sm')

# Textos de ejemplo y sus etiquetas de sentimiento
textos = DFFB2['Texto_Procesado'].astype(str).tolist()
sentimientos = DFFB2['Sentimiento'].astype(str).tolist()

# Preprocesar los textos
def preprocesar_texto(texto):
    doc = nlp(texto)
    return " ".join([token.lemma_ for token in doc if not token.is_stop])

textos_procesados = [preprocesar_texto(texto) for texto in textos]

# Vectorizar los textos
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(textos_procesados)
y = sentimientos

# Dividir los datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Entrenar un modelo SVM
modelo = SVC(kernel='linear')
modelo.fit(X_train, y_train)

# Evaluar el modelo
y_pred = modelo.predict(X_test)
print("Reporte de clasificación:\n", classification_report(y_test, y_pred))
print("Precisión:", accuracy_score(y_test, y_pred))

from transformers import pipeline

# Cargar el pipeline de análisis de sentimiento
sentiment_analysis = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# Textos de ejemplo
textos = DFIN['contenido'].astype(str).tolist()

# Analizar el sentimiento de los textos
resultados = sentiment_analysis(textos)

# Mostrar los resultados
for texto, resultado in zip(textos, resultados):
    print(f"Texto: {texto}")
    print(f"Sentimiento: {resultado['label']} (Score: {resultado['score']:.2f})\n")

"""### Instagram"""

#---------------- Obtener sentimiento dependiendo del comentario en cada DF------------------
from textblob import TextBlob
from deep_translator import GoogleTranslator

# Función para obtener el sentimiento
def obtener_sentimiento(texto):
    try:
        texto_traducido = GoogleTranslator(source='auto', target='en').translate(texto)
        polaridad = TextBlob(texto_traducido).sentiment.polarity
    except:
        # polaridad = TextBlob(texto).sentiment.polarity
        polaridad = 0
        # return 'Neutral'

    if polaridad > 0:
        return 'Positivo'
    elif polaridad == 0:
        return 'Positivo'
    else:
        return 'Negativo'

# Aplicar la función a la columna 'Texto_Procesado' y crear una nueva columna 'Sentimiento'
# DFFB.drop('Sentimiento', axis=1, inplace=True)
DFIN['Sentimiento'] = DFIN['contenido'].apply(obtener_sentimiento)
DFIN2 = DFIN[['Texto_Procesado','Sentimiento']]
DFIN

import spacy
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

# Cargar el modelo en español
nlp = spacy.load('es_core_news_sm')

# Textos de ejemplo y sus etiquetas de sentimiento
textos = DFIN2['Texto_Procesado'].astype(str).tolist()
sentimientos = DFIN2['Sentimiento'].astype(str).tolist()

# Preprocesar los textos
def preprocesar_texto(texto):
    doc = nlp(texto)
    return " ".join([token.lemma_ for token in doc if not token.is_stop])

textos_procesados = [preprocesar_texto(texto) for texto in textos]

# Vectorizar los textos
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(textos_procesados)
y = sentimientos

# Dividir los datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Entrenar un modelo SVM
modelo = SVC(kernel='linear')
modelo.fit(X_train, y_train)

# Evaluar el modelo
y_pred = modelo.predict(X_test)
print("Reporte de clasificación:\n", classification_report(y_test, y_pred))
print("Precisión:", accuracy_score(y_test, y_pred))

from transformers import pipeline

# Cargar el pipeline de análisis de sentimiento
sentiment_analysis = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# Textos de ejemplo
textos = DFFB['contenido'].astype(str).tolist()

# Analizar el sentimiento de los textos
resultados = sentiment_analysis(textos)

# Mostrar los resultados
for texto, resultado in zip(textos, resultados):
    print(f"Texto: {texto}")
    print(f"Sentimiento: {resultado['label']} (Score: {resultado['score']:.2f})\n")

"""### X"""

#---------------- Obtener sentimiento dependiendo del comentario en cada DF------------------
from textblob import TextBlob
from deep_translator import GoogleTranslator

# Función para obtener el sentimiento
def obtener_sentimiento(texto):
    try:
        texto_traducido = GoogleTranslator(source='auto', target='en').translate(texto)
        polaridad = TextBlob(texto_traducido).sentiment.polarity
    except:
        # polaridad = TextBlob(texto).sentiment.polarity
        polaridad = 0
        # return 'Neutral'

    if polaridad > 0:
        return 'Positivo'
    elif polaridad == 0:
        return 'Positivo'
    else:
        return 'Negativo'

# Aplicar la función a la columna 'Texto_Procesado' y crear una nueva columna 'Sentimiento'
# DFFB.drop('Sentimiento', axis=1, inplace=True)
DFX['Sentimiento'] = DFX['contenido'].apply(obtener_sentimiento)
DFX2 = DFX[['Texto_Procesado','Sentimiento']]
DFX

import spacy
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

# Cargar el modelo en español
nlp = spacy.load('es_core_news_sm')

# Textos de ejemplo y sus etiquetas de sentimiento
textos = DFX2['Texto_Procesado'].astype(str).tolist()
sentimientos = DFX2['Sentimiento'].astype(str).tolist()

# Preprocesar los textos
def preprocesar_texto(texto):
    doc = nlp(texto)
    return " ".join([token.lemma_ for token in doc if not token.is_stop])

textos_procesados = [preprocesar_texto(texto) for texto in textos]

# Vectorizar los textos
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(textos_procesados)
y = sentimientos

# Dividir los datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Entrenar un modelo SVM
modelo = SVC(kernel='linear')
modelo.fit(X_train, y_train)

# Evaluar el modelo
y_pred = modelo.predict(X_test)
print("Reporte de clasificación:\n", classification_report(y_test, y_pred))
print("Precisión:", accuracy_score(y_test, y_pred))

from transformers import pipeline

# Cargar el pipeline de análisis de sentimiento
sentiment_analysis = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# Textos de ejemplo
textos = DFIN['contenido'].astype(str).tolist()

# Analizar el sentimiento de los textos
resultados = sentiment_analysis(textos)

# Mostrar los resultados
for texto, resultado in zip(textos, resultados):
    print(f"Texto: {texto}")
    print(f"Sentimiento: {resultado['label']} (Score: {resultado['score']:.2f})\n")

"""### Interpretacion de Tendencias"""

DFFB2 = DFFB[['Texto_Procesado', 'Sentimiento', 'fecha']]

#----------------- Tendencias en Facebook-----------------------
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud

# Cargar los datos (Asegúrate de que DFFB2 ya está cargado en tu entorno)
DFFB2['fecha'] = pd.to_datetime(DFFB2['fecha'])  # Convertir fechas a formato datetime
DFFB2['año_mes'] = DFFB2['fecha'].dt.to_period('M')  # Agrupar por mes

# ---------------------- Tendencia de Sentimientos a lo largo del tiempo ----------------------
tendencia_sentimiento = DFFB2.groupby(['año_mes', 'Sentimiento']).size().unstack()

plt.figure(figsize=(10, 5))
tendencia_sentimiento.plot(kind='line', marker='o', figsize=(10, 5))
plt.title("Tendencia de Sentimientos a lo largo del tiempo")
plt.xlabel("Fecha")
plt.ylabel("Cantidad de publicaciones")
plt.legend(title="Sentimiento")
plt.xticks(rotation=45)
plt.show()

DFIN2 = DFIN[['Texto_Procesado', 'Sentimiento', 'fecha']]

#----------------- Tendencias en Intagram-----------------------
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction import _stop_words

# Cargar los datos (Asegúrate de que DFFB2 ya está cargado en tu entorno)
DFIN2['fecha'] = pd.to_datetime(DFFB2['fecha'])  # Convertir fechas a formato datetime
DFIN2['año_mes'] = DFIN2['fecha'].dt.to_period('M')  # Agrupar por mes

# ---------------------- Tendencia de Sentimientos a lo largo del tiempo ----------------------
tendencia_sentimiento = DFIN2.groupby(['año_mes', 'Sentimiento']).size().unstack()

plt.figure(figsize=(10, 5))
tendencia_sentimiento.plot(kind='line', marker='o', figsize=(10, 5))
plt.title("Tendencia de Sentimientos a lo largo del tiempo")
plt.xlabel("Fecha")
plt.ylabel("Cantidad de publicaciones")
plt.legend(title="Sentimiento")
plt.xticks(rotation=45)
plt.show()

DFX2 = DFX[['Texto_Procesado', 'Sentimiento', 'fecha']]

#----------------- Tendencias en X-----------------------
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud

# Cargar los datos
DFX2['fecha'] = pd.to_datetime(DFX2['fecha'])  # Convertir fechas a formato datetime
DFX2['año_mes'] = DFX2['fecha'].dt.to_period('M')  # Agrupar por mes

# ---------------------- Tendencia de Sentimientos a lo largo del tiempo ----------------------
tendencia_sentimiento = DFX2.groupby(['año_mes', 'Sentimiento']).size().unstack()

plt.figure(figsize=(10, 5))
tendencia_sentimiento.plot(kind='line', marker='o', figsize=(10, 5))
plt.title("Tendencia de Sentimientos a lo largo del tiempo")
plt.xlabel("Fecha")
plt.ylabel("Cantidad de publicaciones")
plt.legend(title="Sentimiento")
plt.xticks(rotation=45)
plt.show()